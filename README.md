# Ollama Python - Testes com IA Local

Este projeto contÃ©m exemplos prÃ¡ticos de como usar a biblioteca Python do Ollama para interagir com modelos de IA locais, especificamente o modelo **Gemma 3**.

## ğŸ“‹ Sobre o Projeto

Este repositÃ³rio foi criado para testar e demonstrar diferentes formas de interagir com modelos de IA locais usando o Ollama. O projeto foca no modelo Gemma 3 e apresenta vÃ¡rias implementaÃ§Ãµes de chat, desde versÃµes simples atÃ© versÃµes mais avanÃ§adas com streaming e histÃ³rico de conversas.

## ğŸš€ PrÃ©-requisitos

- Python 3.8+
- Ollama instalado e rodando localmente
- Modelo Gemma 3 baixado no Ollama

### InstalaÃ§Ã£o do Ollama

```bash
# Instalar o Ollama (Linux)
curl -fsSL https://ollama.ai/install.sh | sh

# Baixar o modelo Gemma 3
ollama pull gemma3
```

## ğŸ“¦ InstalaÃ§Ã£o

1. Clone este repositÃ³rio:
```bash
git clone <url-do-repositorio>
cd ollama_py
```

2. Crie um ambiente virtual:
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows
```

3. Instale as dependÃªncias:
```bash
pip install ollama
```

## ğŸ“ Estrutura do Projeto

- `chat.py` - ImplementaÃ§Ã£o bÃ¡sica de chat com resposta completa
- `chat-stream.py` - Chat com streaming (resposta em tempo real)
- `chat-async.py` - Chat assÃ­ncrono com streaming
- `chat-with-history.py` - Chat com histÃ³rico de conversas mantido

## ğŸ¯ Como Usar

### Chat BÃ¡sico
```bash
python chat.py
```
ImplementaÃ§Ã£o simples que faz uma pergunta e retorna a resposta completa.

### Chat com Streaming
```bash
python chat-stream.py
```
Mostra a resposta sendo escrita em tempo real, caracterÃ­stica por caracterÃ­stica.

### Chat AssÃ­ncrono
```bash
python chat-async.py
```
VersÃ£o assÃ­ncrona do chat com streaming, ideal para aplicaÃ§Ãµes que precisam de melhor performance.

### Chat com HistÃ³rico
```bash
python chat-with-history.py
```
MantÃ©m o contexto da conversa, permitindo perguntas de follow-up que fazem referÃªncia a mensagens anteriores.

## ğŸ”§ CaracterÃ­sticas

- **Modelo**: Gemma 3 (IA local)
- **Streaming**: Respostas em tempo real
- **HistÃ³rico**: ManutenÃ§Ã£o de contexto entre mensagens
- **AssÃ­ncrono**: Suporte a operaÃ§Ãµes nÃ£o-bloqueantes
- **Simplicidade**: CÃ³digo limpo e fÃ¡cil de entender

## ğŸ’¡ Exemplos de Uso

Todos os scripts seguem o mesmo padrÃ£o:
1. Solicitam uma pergunta do usuÃ¡rio
2. Enviam para o modelo Gemma 3
3. Exibem a resposta (com ou sem streaming)
4. No caso do chat com histÃ³rico, mantÃªm o contexto para prÃ³ximas perguntas

## ğŸ› ï¸ PersonalizaÃ§Ã£o

VocÃª pode facilmente modificar os scripts para:
- Usar outros modelos do Ollama (trocar `'gemma3'` por outro modelo)
- Alterar o prompt padrÃ£o
- Adicionar validaÃ§Ãµes de entrada
- Implementar persistÃªncia do histÃ³rico em arquivo

## ğŸ“ Notas

- Certifique-se de que o Ollama estÃ¡ rodando antes de executar os scripts
- O modelo Gemma 3 deve estar baixado localmente
- Para melhor performance, use o chat assÃ­ncrono em aplicaÃ§Ãµes web

## ğŸ¤ ContribuiÃ§Ãµes

Sinta-se Ã  vontade para contribuir com melhorias, novos exemplos ou correÃ§Ãµes!

## ğŸ“„ LicenÃ§a

Este projeto Ã© para fins educacionais e de teste. Use conforme necessÃ¡rio.
